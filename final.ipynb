{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anushrijamar/Machine_learning/blob/main/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx3fmASUHbjS"
      },
      "source": [
        "decision tree, knn, svm, linear regression, random forest, xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWP2H4OwCWOV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor,plot_tree\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import sys\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbBakdrFCbvw"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/BTC-USD.csv')\n",
        "data.head()\n",
        "from scipy.stats import pearsonr\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'data' is your DataFrame with columns 'Open' and 'Close'\n",
        "# Replace 'Open' and 'Close' with your actual column names\n",
        "\n",
        "# Calculate Pearson correlation coefficient and p-value\n",
        "corr_open, p_value = pearsonr(data['Open'], data['Close'])\n",
        "corr_low, p_value = pearsonr(data['Low'], data['Close'])\n",
        "corr_high, p_value = pearsonr(data['High'], data['Close'])\n",
        "corr_vol, p_value = pearsonr(data['Volume'], data['Close'])\n",
        "# Print the results\n",
        "print(f\"Pearson correlation coefficient for Open vs Close: {corr_open:.4f}\")\n",
        "print(f\"Pearson correlation coefficient for Low vs Close: {corr_low:.4f}\")\n",
        "print(f\"Pearson correlation coefficient for High vs Close: {corr_high:.4f}\")\n",
        "print(f\"Pearson correlation coefficient for Vol vs Close: {corr_vol:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "correlation_matrix = data.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Assuming data has already been read using pd.read_csv('/content/BTC-USD.csv')\n",
        "\n",
        "date = data['Date']\n",
        "close = data['Close']\n",
        "open_price = data['Open']\n",
        "high = data['High']\n",
        "low = data['Low']\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n",
        "\n",
        "# Plot Open vs Close\n",
        "ax1.plot(date, open_price, color=\"blue\", linestyle='-', label='Open')\n",
        "ax1.plot(date, close, color=\"red\", linestyle='-', label='Close')\n",
        "ax1.set_ylabel('Price')\n",
        "ax1.set_title('Open vs Close')\n",
        "\n",
        "# Plot High vs Close\n",
        "ax2.plot(date, high, color=\"green\", linestyle='-', label='High')\n",
        "ax2.plot(date, close, color=\"red\", linestyle='-', label='Close')\n",
        "ax2.set_ylabel('Price')\n",
        "ax2.set_title('High vs Close')\n",
        "\n",
        "# Plot Low vs Close\n",
        "ax3.plot(date, low, color=\"orange\", linestyle='-', label='Low')\n",
        "ax3.plot(date, close, color=\"red\", linestyle='-', label='Close')\n",
        "ax3.set_xlabel('Date')\n",
        "ax3.set_ylabel('Price')\n",
        "ax3.set_title('Low vs Close')\n",
        "\n",
        "# Add legend to each subplot\n",
        "ax1.legend()\n",
        "ax2.legend()\n",
        "ax3.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "data=data.drop(data.columns[0],axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hwmMN_xCoad"
      },
      "outputs": [],
      "source": [
        "data=data.dropna()\n",
        "data=data.copy()\n",
        "data\n",
        "x=data.iloc[:,0:3].join(data.iloc[:,4:5])\n",
        "y=data.iloc[:,3]\n",
        "print(x)\n",
        "print(y)\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3 ,random_state=21)\n",
        "sc=StandardScaler()\n",
        "x_train_scaled=sc.fit_transform(x_train)\n",
        "x_test_scaled=sc.transform(x_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-1idMJl_N0X"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "correlation_matrix = data.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIxxH7m8H9Oq"
      },
      "outputs": [],
      "source": [
        "data.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbeq8R9-H5Lk"
      },
      "source": [
        "decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGJLdzE_Cd02"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "import sys\n",
        "\n",
        "# Assuming x_train_scaled, x_test_scaled, y_train, y_test are already defined\n",
        "\n",
        "rscores = []\n",
        "mae= []\n",
        "final_r = []\n",
        "final_mae = []\n",
        "maxn = -sys.maxsize\n",
        "best_model = None #based on maximum r2 score\n",
        "mse=sys.maxsize\n",
        "for i in [\"absolute_error\", \"poisson\", \"friedman_mse\", \"squared_error\"]: #hyperparameter\n",
        "    for j in range(10, 60, 10): #depth\n",
        "        dtree = DecisionTreeRegressor(criterion=i, random_state=42, max_depth=j)\n",
        "        dtree.fit(x_train_scaled, y_train)\n",
        "        y_preddes = dtree.predict(x_test_scaled)\n",
        "        error = r2_score(y_test, y_preddes)\n",
        "        rscores.append(error)\n",
        "        m=mean_absolute_error(y_test,y_preddes)\n",
        "        mae.append(m)\n",
        "        if maxn < error:\n",
        "            maxn = error\n",
        "            algo = i\n",
        "            depth = j\n",
        "            best_model = dtree\n",
        "            mse=m\n",
        "\n",
        "# print(\"For DecisionTree Regressor, the maximum r2 score found for\", algo, \"with\", maxn, \"and depth\", depth)\n",
        "print(rscores)\n",
        "print(mae)\n",
        "final_r.append(maxn)\n",
        "final_mae.append(mse)\n",
        "\n",
        "# Print the optimal algorithm\n",
        "print(\"Optimal Algorithm: \", algo)\n",
        "print(\"R2 score: \", maxn)\n",
        "print(\"Optimal Depth: \", depth)\n",
        "print(\"Mean absolute error : \",mse)\n",
        "# Plot best fit line and scatter plot / best model would have all points on the red line\n",
        "plt.scatter(y_test, y_preddes, color='blue', label='Actual vs. Predicted')\n",
        "plt.plot([np.min(y_test), np.max(y_test)], [np.min(y_test), np.max(y_test)], linestyle='--', color='red', linewidth=2, label='Best Fit Line')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Actual vs. Predicted Values')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "depth = [10,20,30,40,50,10,20,30,40,50,10,20,30,40,50,10,20,30,40,50]\n",
        "#depth = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
        "# plt.plot(depth, rscores, marker='o')\n",
        "# plt.xlabel('Depth of Decision Tree')\n",
        "# plt.ylabel('R2 Score')\n",
        "# plt.title('Depth vs. R2 Score')\n",
        "# plt.show()\n",
        "# plt.plot(depth, mae, marker='o')\n",
        "# plt.xlabel('Depth of Decision Tree')\n",
        "# plt.ylabel('Mean Absolute Error')\n",
        "# plt.title('Depth vs. Mean Absolute Error')\n",
        "# plt.show()\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "color = 'red'\n",
        "ax1.set_xlabel('Depth of Decision Tree')\n",
        "ax1.set_ylabel('R2 Score', color=color)\n",
        "ax1.scatter(depth, rscores, marker='o', color=color)\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "color = 'blue'\n",
        "ax2.set_ylabel('Mean Absolute Error', color=color)\n",
        "ax2.scatter(depth, mae, marker='x', color=color)\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.title('Depth vs. R2 Score and Mean Absolute Error')\n",
        "plt.show()\n",
        "# depth = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(depth, rscores, label='R2 Score', marker='o')\n",
        "plt.plot(depth, mae, label='MSE', marker='x')\n",
        "plt.xlabel('Depth of Decision Tree')\n",
        "plt.ylabel('Score')\n",
        "plt.title('R2 Score and MSE vs. Depth of Decision Tree')\n",
        "plt.xticks(depth)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PV55lHuyY4J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_QqG2khDpyG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "param_grid = {'n_neighbors': range(1, 21, 2)}\n",
        "knn = KNeighborsRegressor()\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=4, scoring='neg_mean_absolute_error')\n",
        "grid_search.fit(x_train_scaled, y_train)\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best mean absolute error:\", -grid_search.best_score_)\n",
        "n=grid_search.best_params_['n_neighbors']\n",
        "knn=KNeighborsRegressor(n_neighbors=n)\n",
        "knn.fit(x_train_scaled,y_train)\n",
        "y_predknn=knn.predict(x_test_scaled)\n",
        "r2=r2_score(y_test,y_predknn)\n",
        "print(\"R2 score: \",r2)\n",
        "final_r.append(r2)\n",
        "plt.scatter(y_test, y_predknn, color='red', alpha=0.7, label='Actual vs Predicted')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], linestyle='--', color='blue', linewidth=2, label='Perfect Prediction')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title(' KNN Actual vs Predicted')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "final_mae.append(mean_absolute_error(y_test,y_predknn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3AZuT0IJYFj"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "import sys\n",
        "scores=[]\n",
        "optimal_n=8\n",
        "max=-sys.maxsize\n",
        "for i in range(1,50,1):\n",
        "  knn=KNeighborsRegressor(n_neighbors=i)\n",
        "  knn.fit(x_train_scaled,y_train)\n",
        "  y_predknn=knn.predict(x_test_scaled)\n",
        "  s=r2_score(y_test,y_predknn)\n",
        "  scores.append(s)\n",
        "  if(s>max):\n",
        "    max=s\n",
        "    optimal_n=i\n",
        "print(\"R2 score is \",max,\" and the n optimal nieghbors is \",optimal_n)\n",
        "print(scores)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1, 50, 1), scores, marker='o')\n",
        "plt.xlabel('Number of Neighbors (n_neighbors)')\n",
        "plt.ylabel('R2 Score')\n",
        "plt.title('Effect of Number of Neighbors on R2 Score')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJn6kljqDuhh"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "lr=LinearRegression()\n",
        "lr.fit(x_train_scaled,y_train)\n",
        "y_predlr=lr.predict(x_test_scaled)\n",
        "r2s=r2_score(y_test,y_predlr)\n",
        "print(\"R2 score: \",r2s)\n",
        "print(\"mean absolute error\", mean_absolute_error(y_test,y_predlr))\n",
        "plt.scatter(y_test, y_predlr, color='red', alpha=0.5, label='Actual vs Predicted')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], linestyle='--', color='blue', linewidth=2, label='Perfect Prediction')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predcited')\n",
        "plt.title(' Linear Regression Actual vs Predicted')\n",
        "final_r.append(r2s)\n",
        "final_mae.append(mean_absolute_error(y_test,y_predlr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJrpbzq6Dun9"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from numpy import mean, absolute\n",
        "optimal_n=1\n",
        "optimal_mae=sys.maxsize\n",
        "optimal_r=-sys.maxsize\n",
        "for i in range(2,100,10):\n",
        " rndomforest = RandomForestRegressor(n_estimators=i, random_state=42)\n",
        " rndomforest.fit(x_train_scaled, y_train)\n",
        " y_predrndom = rndomforest.predict(x_test_scaled)\n",
        "\n",
        " mserndom = mean_absolute_error(y_test, y_predrndom)\n",
        " rs = r2_score(y_test, y_predrndom)\n",
        " if optimal_r<rs and optimal_mae>mserndom :\n",
        "  optimal_r=rs\n",
        "  optimal_n=i\n",
        "  optimal_mae=mserndom\n",
        "print(f\"Mean absolute Error: {optimal_mae: .2f}\")\n",
        "print(\"r2 score is : \", optimal_r)\n",
        "print(\"optimal number of Trees \",optimal_n)\n",
        "min_mean_absolute_error = sys.maxsize\n",
        "optimal_folds = 0\n",
        "optimal_r2 = 0\n",
        "\n",
        "for i in range(2, 50, 10):\n",
        "    cv = KFold(n_splits=i, random_state=1, shuffle=True)\n",
        "    model = RandomForestRegressor()\n",
        "    scores = cross_val_score(model, x_train_scaled, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
        "\n",
        "    # Calculate the mean absolute error for the current number of folds\n",
        "    current_mean_absolute_error = mean(absolute(scores))\n",
        "\n",
        "    # Fit the model on the training data\n",
        "    model.fit(x_train_scaled, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_predR = model.predict(x_test_scaled)\n",
        "\n",
        "    # Calculate R2 score for the current model\n",
        "    current_r2 = r2_score(y_test, y_predR)\n",
        "    m=mean_absolute_error(y_test,y_predR)\n",
        "    # Consider a trade-off between mean absolute error and R2 score\n",
        "    # Adjust the coefficients in the condition based on your preference\n",
        "    if 0.8 * current_mean_absolute_error + 0.2 * current_r2 < 0.8 * min_mean_absolute_error + 0.2 * optimal_r2:\n",
        "        min_mean_absolute_error = current_mean_absolute_error\n",
        "        optimal_folds = i\n",
        "        optimal_r2 = current_r2\n",
        "        mae=m\n",
        "\n",
        "print(\"CV k-optimal folds : \", optimal_folds)\n",
        "plt.scatter(y_test, y_predrndom, color='green', label='Actual vs Predicted', alpha=0.5)\n",
        "plt.plot([np.min(y_test), np.max(y_test)], [np.min(y_test), np.max(y_test)], color='yellow', linestyle='--', linewidth=2, label='Ideal Line')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title(\"Random Forest\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "final_r.append(optimal_r)\n",
        "final_mae.append(optimal_mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLdZ583fD3s7"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "regressor = SVR(kernel='linear', C=1)\n",
        "regressor.fit(x_train_scaled, y_train)\n",
        "y_predsvm = regressor.predict(x_test_scaled)\n",
        "print(\"R2 score\",r2_score(y_test,y_predsvm))\n",
        "print(\"Absolute error \",mean_absolute_error(y_test,y_predsvm))\n",
        "plt.scatter(y_test, y_predsvm, color='green', label='Actual vs Predicted', alpha=0.5)\n",
        "plt.plot([np.min(y_test), np.max(y_test)], [np.min(y_test), np.max(y_test)], color='yellow', linestyle='--', linewidth=2, label='Ideal Line')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title(\"Support Vector Regressor\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "final_r.append(r2_score(y_test,y_predsvm))\n",
        "final_mae.append(mean_absolute_error(y_test,y_predsvm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmj8_m16u9SS"
      },
      "source": [
        "XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnt_ooeYDNzY"
      },
      "outputs": [],
      "source": [
        "pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjpC_iPeC9Le"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "import sys\n",
        "opt_n=1\n",
        "opt_mae=sys.maxsize\n",
        "opt_r=-sys.maxsize\n",
        "for i in range(2,150,10):\n",
        " regressorxgb = XGBRegressor(objective='reg:squarederror', n_estimators=i, random_state=42)\n",
        "\n",
        " regressorxgb.fit(x_train_scaled, y_train)\n",
        "\n",
        " y_predxgb = regressorxgb.predict(x_test_scaled)\n",
        "\n",
        "\n",
        " msexgb = mean_absolute_error(y_test, y_predxgb)\n",
        " xg_r=r2_score(y_test,y_predxgb)\n",
        " if opt_r<xg_r and opt_mae>msexgb :\n",
        "  opt_r=xg_r\n",
        "  opt_n=i\n",
        "  opt_mae=msexgb\n",
        "print(f\"Mean absolute Error: {opt_mae:.2f}\")\n",
        "print(\"R2 score \",opt_r)\n",
        "print(\"n estimators \",opt_n)\n",
        "plt.scatter(y_test, y_predxgb, color='red', alpha=0.5, label='Actual vs Predicted')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], linestyle='--', color='blue', linewidth=2, label='Perfect Prediction')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predcited')\n",
        "plt.title(' XGBoost Actual vs Predicted')\n",
        "final_r.append(opt_r)\n",
        "final_mae.append(opt_mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImZJ0lGtiq_v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjY-7vEEisAK"
      },
      "outputs": [],
      "source": [
        "#EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxbRP8N7iyAP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "algorithms = ['DecisonT', 'KNN', 'LinearR', 'RandomF','SVR','XGB']\n",
        "fil=  [value for value in final_r if value != 0]\n",
        "\n",
        "print(fil)\n",
        "\n",
        "plt.bar(algorithms, fil, color='blue')\n",
        "\n",
        "plt.xlabel('Algorithms')\n",
        "plt.ylabel('r2_score')\n",
        "plt.title('Bar Graph Example')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.bar(algorithms, final_mae, color='red')\n",
        "\n",
        "plt.xlabel('Algorithms')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Bar Graph ')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt7oIMrQe1DL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "algorithms = ['DecisonT', 'KNN', 'LinearR', 'RandomF', 'SVR', 'XGB']\n",
        "fil = [value * 1000 for value in final_r if value != 0]  # Multiply by 1000 to scale the y-axis\n",
        "final_mae = [value for value in final_mae if value != 0]  # Replace with your actual data for Mean Squared Error\n",
        "final_mae = [value / 100 for value in final_mae if value != 0]\n",
        "plt.bar(algorithms, fil, color='blue')\n",
        "\n",
        "plt.xlabel('Algorithms')\n",
        "plt.ylabel('R2 Score (scaled to 0.001)')\n",
        "plt.title('Bar Graph Example')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.bar(algorithms, final_mae, color='red')\n",
        "\n",
        "plt.xlabel('Algorithms')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Bar Graph ')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyLeI3SWFszj"
      },
      "outputs": [],
      "source": [
        "user_input = input(\"Enter values for Open, High, Low, Adj Close (comma-separated): \")\n",
        "#20482,20742,20087,20159,55552169483\n",
        "user_input = [float(x) for x in user_input.split(',')]\n",
        "predicted_close = regressorxgb.predict([user_input])\n",
        "print(f\"Predicted Close Price: {predicted_close[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzT8I8qefco9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}